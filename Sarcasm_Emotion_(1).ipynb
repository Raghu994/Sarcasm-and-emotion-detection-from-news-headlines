{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "COMPLETE FIXED Multi-Task RoBERTa Model for Sarcasm Detection and Emotion Classification\n",
        "Optimized for better emotion learning with simplified architecture\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import RobertaTokenizer, RobertaModel, get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import (accuracy_score, precision_recall_fscore_support,\n",
        "                             confusion_matrix, classification_report, roc_auc_score)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"FIXED MULTI-TASK ROBERTA: SARCASM + EMOTION CLASSIFICATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Check device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"\\nUsing device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# ============================================================================\n",
        "# 1. LOAD AND PREPARE DATA WITH EMOTION GROUPING\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"1. LOADING AND PREPARING DATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv('/content/final_clean_no_duplicates.csv')  # MODIFY THIS PATH\n",
        "print(f\"Loaded dataset: {df.shape[0]} rows\")\n",
        "\n",
        "# Text preprocessing\n",
        "print(\"\\nPreprocessing text...\")\n",
        "df['headline_clean'] = df['headline'].str.strip()\n",
        "\n",
        "# Define emotion grouping (27 → 8 classes)\n",
        "emotion_mapping = {\n",
        "    # Positive emotions\n",
        "    'admiration': 'positive',\n",
        "    'excitement': 'positive',\n",
        "    'hope': 'positive',\n",
        "    'joy': 'positive',\n",
        "    'love': 'positive',\n",
        "    'empowerment': 'positive',\n",
        "    'relaxation': 'positive',\n",
        "\n",
        "    # Negative emotions\n",
        "    'anger': 'negative',\n",
        "    'contempt': 'negative',\n",
        "    'disapproval': 'negative',\n",
        "    'embarrassment': 'negative',\n",
        "    'fear': 'negative',\n",
        "    'sadness': 'negative',\n",
        "    'concern': 'negative',\n",
        "\n",
        "    # Complex/Neutral emotions\n",
        "    'activism': 'complex',\n",
        "    'anticipation': 'complex',\n",
        "    'curiosity': 'complex',\n",
        "    'drama': 'complex',\n",
        "    'intensity': 'complex',\n",
        "    'reflection': 'complex',\n",
        "    'solidarity': 'complex',\n",
        "    'sincerity': 'complex',\n",
        "\n",
        "    # Specific categories\n",
        "    'humor': 'humor',\n",
        "    'somber irony': 'irony',\n",
        "    'surprise': 'surprise',\n",
        "    'neutral': 'neutral',\n",
        "    'nostalgia': 'nostalgia'\n",
        "}\n",
        "\n",
        "# Apply emotion grouping\n",
        "df['emotion_grouped'] = df['emotion'].map(emotion_mapping)\n",
        "\n",
        "# Check for any unmapped emotions\n",
        "unmapped = df[df['emotion_grouped'].isnull()]['emotion'].unique()\n",
        "if len(unmapped) > 0:\n",
        "    print(f\"Warning: Unmapped emotions: {unmapped}\")\n",
        "    # Fill unmapped with 'complex'\n",
        "    df['emotion_grouped'] = df['emotion_grouped'].fillna('complex')\n",
        "\n",
        "# Encode grouped emotion labels\n",
        "label_encoder = LabelEncoder()\n",
        "df['emotion_encoded'] = label_encoder.fit_transform(df['emotion_grouped'])\n",
        "emotion_classes = label_encoder.classes_\n",
        "num_emotions = len(emotion_classes)\n",
        "\n",
        "print(f\"\\nTarget Variables:\")\n",
        "print(f\"  - is_sarcastic: Binary (0/1)\")\n",
        "print(f\"  - emotion: {num_emotions} classes (reduced from 27)\")\n",
        "print(f\"  Emotion classes: {list(emotion_classes)}\")\n",
        "\n",
        "# Display emotion distribution\n",
        "print(f\"\\nEmotion Distribution:\")\n",
        "emotion_counts = df['emotion_grouped'].value_counts()\n",
        "for emotion, count in emotion_counts.items():\n",
        "    percentage = (count / len(df)) * 100\n",
        "    print(f\"  {emotion:15s}: {count:4d} samples ({percentage:.1f}%)\")\n",
        "\n",
        "# ============================================================================\n",
        "# 2. DATA SPLITTING\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"2. CREATING DATA SPLITS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "X = df['headline_clean'].values\n",
        "y_sarcasm = df['is_sarcastic'].values\n",
        "y_emotion = df['emotion_encoded'].values\n",
        "\n",
        "# Create combined labels for better stratification\n",
        "combined_labels = [f\"{sarc}_{emo}\" for sarc, emo in zip(y_sarcasm, y_emotion)]\n",
        "\n",
        "# Split data\n",
        "X_train, X_temp, y_sarcasm_train, y_sarcasm_temp, y_emotion_train, y_emotion_temp = train_test_split(\n",
        "    X, y_sarcasm, y_emotion,\n",
        "    test_size=0.2,\n",
        "    random_state=SEED,\n",
        "    stratify=combined_labels\n",
        ")\n",
        "\n",
        "X_val, X_test, y_sarcasm_val, y_sarcasm_test, y_emotion_val, y_emotion_test = train_test_split(\n",
        "    X_temp, y_sarcasm_temp, y_emotion_temp,\n",
        "    test_size=0.5,\n",
        "    random_state=SEED,\n",
        "    stratify=[f\"{s}_{e}\" for s, e in zip(y_sarcasm_temp, y_emotion_temp)]\n",
        ")\n",
        "\n",
        "print(f\"Dataset Split:\")\n",
        "print(f\"  Train set: {len(X_train)} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
        "print(f\"  Val set:   {len(X_val)} samples ({len(X_val)/len(X)*100:.1f}%)\")\n",
        "print(f\"  Test set:  {len(X_test)} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
        "\n",
        "# ============================================================================\n",
        "# 3. TOKENIZATION\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"3. TOKENIZING WITH ROBERTA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"Loading RoBERTa tokenizer...\")\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "MAX_LENGTH = 128\n",
        "print(f\"Max sequence length: {MAX_LENGTH}\")\n",
        "\n",
        "class HeadlineDataset(Dataset):\n",
        "    def __init__(self, texts, sarcasm_labels, emotion_labels, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.sarcasm_labels = sarcasm_labels\n",
        "        self.emotion_labels = emotion_labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        sarcasm = self.sarcasm_labels[idx]\n",
        "        emotion = self.emotion_labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'sarcasm_label': torch.tensor(sarcasm, dtype=torch.long),\n",
        "            'emotion_label': torch.tensor(emotion, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "print(\"Creating datasets...\")\n",
        "train_dataset = HeadlineDataset(X_train, y_sarcasm_train, y_emotion_train, tokenizer, MAX_LENGTH)\n",
        "val_dataset = HeadlineDataset(X_val, y_sarcasm_val, y_emotion_val, tokenizer, MAX_LENGTH)\n",
        "test_dataset = HeadlineDataset(X_test, y_sarcasm_test, y_emotion_test, tokenizer, MAX_LENGTH)\n",
        "\n",
        "BATCH_SIZE = 32  # Increased batch size for stability\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Training batches: {len(train_loader)}\")\n",
        "print(f\"Validation batches: {len(val_loader)}\")\n",
        "print(f\"Test batches: {len(test_loader)}\")\n",
        "\n",
        "# ============================================================================\n",
        "# 4. SIMPLIFIED MODEL ARCHITECTURE\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"4. BUILDING SIMPLIFIED MULTI-TASK MODEL\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "class SimpleMultiTaskRoBERTa(nn.Module):\n",
        "    def __init__(self, num_emotions, dropout=0.1):\n",
        "        super(SimpleMultiTaskRoBERTa, self).__init__()\n",
        "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Simple direct classification heads\n",
        "        self.sarcasm_classifier = nn.Linear(768, 2)\n",
        "        self.emotion_classifier = nn.Linear(768, num_emotions)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.roberta(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        pooled_output = outputs.pooler_output\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "\n",
        "        sarcasm_logits = self.sarcasm_classifier(pooled_output)\n",
        "        emotion_logits = self.emotion_classifier(pooled_output)\n",
        "\n",
        "        return sarcasm_logits, emotion_logits\n",
        "\n",
        "print(\"Loading simplified RoBERTa model...\")\n",
        "model = SimpleMultiTaskRoBERTa(num_emotions=num_emotions).to(device)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"\\nTotal parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "# ============================================================================\n",
        "# 5. OPTIMIZED TRAINING SETUP\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"5. TRAINING CONFIGURATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "EPOCHS = 6\n",
        "INITIAL_EMOTION_WEIGHT = 0.8  # Start with focus on emotion\n",
        "INITIAL_SARCASM_WEIGHT = 0.2\n",
        "\n",
        "# Simple loss functions without class weights (for stability)\n",
        "criterion_sarcasm = nn.CrossEntropyLoss()\n",
        "criterion_emotion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer with different learning rates\n",
        "optimizer = AdamW([\n",
        "    {'params': model.roberta.parameters(), 'lr': 2e-5},\n",
        "    {'params': model.sarcasm_classifier.parameters(), 'lr': 1e-4},\n",
        "    {'params': model.emotion_classifier.parameters(), 'lr': 5e-4}  # Higher LR for emotion\n",
        "], weight_decay=0.01)\n",
        "\n",
        "# Learning rate scheduler\n",
        "total_steps = len(train_loader) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=int(0.1 * total_steps),\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "print(f\"Epochs: {EPOCHS}\")\n",
        "print(f\"Initial task weights - Emotion: {INITIAL_EMOTION_WEIGHT}, Sarcasm: {INITIAL_SARCASM_WEIGHT}\")\n",
        "print(f\"Learning rates - RoBERTa: 2e-5, Sarcasm: 1e-4, Emotion: 5e-4\")\n",
        "print(f\"Total training steps: {total_steps}\")\n",
        "\n",
        "# ============================================================================\n",
        "# 6. IMPROVED TRAINING LOOP\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"6. STARTING TRAINING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def train_epoch(model, data_loader, optimizer, scheduler, device, epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    sarcasm_correct = 0\n",
        "    emotion_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    # Dynamic weighting: Gradually balance the tasks\n",
        "    emotion_weight = max(INITIAL_EMOTION_WEIGHT - (epoch * 0.1), 0.4)\n",
        "    sarcasm_weight = 1.0 - emotion_weight\n",
        "\n",
        "    for batch_idx, batch in enumerate(data_loader):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        sarcasm_labels = batch['sarcasm_label'].to(device)\n",
        "        emotion_labels = batch['emotion_label'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        sarcasm_logits, emotion_logits = model(input_ids, attention_mask)\n",
        "\n",
        "        loss_sarcasm = criterion_sarcasm(sarcasm_logits, sarcasm_labels)\n",
        "        loss_emotion = criterion_emotion(emotion_logits, emotion_labels)\n",
        "\n",
        "        # Dynamic weighted loss\n",
        "        loss = sarcasm_weight * loss_sarcasm + emotion_weight * loss_emotion\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        sarcasm_preds = torch.argmax(sarcasm_logits, dim=1)\n",
        "        emotion_preds = torch.argmax(emotion_logits, dim=1)\n",
        "\n",
        "        sarcasm_correct += (sarcasm_preds == sarcasm_labels).sum().item()\n",
        "        emotion_correct += (emotion_preds == emotion_labels).sum().item()\n",
        "        total_samples += sarcasm_labels.size(0)\n",
        "\n",
        "        if (batch_idx + 1) % 50 == 0:\n",
        "            current_sarcasm_acc = sarcasm_correct / total_samples\n",
        "            current_emotion_acc = emotion_correct / total_samples\n",
        "            print(f\"  Batch {batch_idx+1:3d}/{len(data_loader)} | \"\n",
        "                  f\"Loss: {loss.item():.4f} | \"\n",
        "                  f\"Sarcasm: {current_sarcasm_acc:.4f} | \"\n",
        "                  f\"Emotion: {current_emotion_acc:.4f} | \"\n",
        "                  f\"Weights: S({sarcasm_weight:.1f}) E({emotion_weight:.1f})\")\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    sarcasm_acc = sarcasm_correct / total_samples\n",
        "    emotion_acc = emotion_correct / total_samples\n",
        "\n",
        "    return avg_loss, sarcasm_acc, emotion_acc, emotion_weight\n",
        "\n",
        "def evaluate(model, data_loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    all_sarcasm_preds = []\n",
        "    all_sarcasm_labels = []\n",
        "    all_sarcasm_probs = []\n",
        "    all_emotion_preds = []\n",
        "    all_emotion_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            sarcasm_labels = batch['sarcasm_label'].to(device)\n",
        "            emotion_labels = batch['emotion_label'].to(device)\n",
        "\n",
        "            sarcasm_logits, emotion_logits = model(input_ids, attention_mask)\n",
        "\n",
        "            loss_sarcasm = criterion_sarcasm(sarcasm_logits, sarcasm_labels)\n",
        "            loss_emotion = criterion_emotion(emotion_logits, emotion_labels)\n",
        "            loss = 0.5 * loss_sarcasm + 0.5 * loss_emotion  # Equal weights for evaluation\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            sarcasm_probs = torch.softmax(sarcasm_logits, dim=1)\n",
        "            sarcasm_preds = torch.argmax(sarcasm_logits, dim=1)\n",
        "            emotion_preds = torch.argmax(emotion_logits, dim=1)\n",
        "\n",
        "            all_sarcasm_preds.extend(sarcasm_preds.cpu().numpy())\n",
        "            all_sarcasm_labels.extend(sarcasm_labels.cpu().numpy())\n",
        "            all_sarcasm_probs.extend(sarcasm_probs[:, 1].cpu().numpy())\n",
        "            all_emotion_preds.extend(emotion_preds.cpu().numpy())\n",
        "            all_emotion_labels.extend(emotion_labels.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    return (avg_loss,\n",
        "            np.array(all_sarcasm_preds), np.array(all_sarcasm_labels), np.array(all_sarcasm_probs),\n",
        "            np.array(all_emotion_preds), np.array(all_emotion_labels))\n",
        "\n",
        "# Training with early stopping\n",
        "best_val_emotion_acc = 0\n",
        "patience = 3\n",
        "patience_counter = 0\n",
        "best_model_state = None\n",
        "\n",
        "training_history = []\n",
        "\n",
        "print(\"\\nStarting training...\")\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"EPOCH {epoch + 1}/{EPOCHS}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    # Training\n",
        "    train_loss, train_sarcasm_acc, train_emotion_acc, current_emotion_weight = train_epoch(\n",
        "        model, train_loader, optimizer, scheduler, device, epoch\n",
        "    )\n",
        "\n",
        "    # Validation\n",
        "    val_loss, val_sarcasm_preds, val_sarcasm_labels, val_sarcasm_probs, val_emotion_preds, val_emotion_labels = evaluate(\n",
        "        model, val_loader, device\n",
        "    )\n",
        "\n",
        "    val_sarcasm_acc = accuracy_score(val_sarcasm_labels, val_sarcasm_preds)\n",
        "    val_emotion_acc = accuracy_score(val_emotion_labels, val_emotion_preds)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
        "    print(f\"  Training   -> Loss: {train_loss:.4f}, Sarcasm: {train_sarcasm_acc:.4f}, Emotion: {train_emotion_acc:.4f}\")\n",
        "    print(f\"  Validation -> Loss: {val_loss:.4f}, Sarcasm: {val_sarcasm_acc:.4f}, Emotion: {val_emotion_acc:.4f}\")\n",
        "    print(f\"  Emotion weight: {current_emotion_weight:.2f}\")\n",
        "\n",
        "    # Save training history\n",
        "    training_history.append({\n",
        "        'epoch': epoch + 1,\n",
        "        'train_loss': train_loss,\n",
        "        'train_sarcasm_acc': train_sarcasm_acc,\n",
        "        'train_emotion_acc': train_emotion_acc,\n",
        "        'val_loss': val_loss,\n",
        "        'val_sarcasm_acc': val_sarcasm_acc,\n",
        "        'val_emotion_acc': val_emotion_acc\n",
        "    })\n",
        "\n",
        "    # Early stopping based on emotion accuracy\n",
        "    if val_emotion_acc > best_val_emotion_acc:\n",
        "        best_val_emotion_acc = val_emotion_acc\n",
        "        patience_counter = 0\n",
        "        best_model_state = model.state_dict().copy()\n",
        "        print(f\"  ✓ New best model! Emotion accuracy: {val_emotion_acc:.4f}\")\n",
        "\n",
        "        # Save model checkpoint\n",
        "        torch.save(model.state_dict(), f'best_model_epoch_{epoch+1}.pt')\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(f\"  × No improvement ({patience_counter}/{patience})\")\n",
        "\n",
        "    if patience_counter >= patience and epoch >= 2:  # Minimum 3 epochs\n",
        "        print(f\"\\nEarly stopping triggered after {epoch + 1} epochs!\")\n",
        "        model.load_state_dict(best_model_state)\n",
        "        break\n",
        "\n",
        "# Load best model for final evaluation\n",
        "if best_model_state is not None:\n",
        "    model.load_state_dict(best_model_state)\n",
        "    print(\"Loaded best model for final evaluation.\")\n",
        "\n",
        "# ============================================================================\n",
        "# 7. FINAL EVALUATION ON TEST SET\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"7. FINAL TEST SET EVALUATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "test_loss, sarcasm_preds, sarcasm_labels, sarcasm_probs, emotion_preds, emotion_labels = evaluate(\n",
        "    model, test_loader, device\n",
        ")\n",
        "\n",
        "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# 8. SARCASM DETECTION METRICS\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"8. SARCASM DETECTION RESULTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "sarcasm_acc = accuracy_score(sarcasm_labels, sarcasm_preds)\n",
        "sarcasm_precision, sarcasm_recall, sarcasm_f1, _ = precision_recall_fscore_support(\n",
        "    sarcasm_labels, sarcasm_preds, average='binary'\n",
        ")\n",
        "sarcasm_auc = roc_auc_score(sarcasm_labels, sarcasm_probs)\n",
        "\n",
        "print(f\"\\nSarcasm Detection Metrics:\")\n",
        "print(f\"  Accuracy:  {sarcasm_acc:.4f}\")\n",
        "print(f\"  Precision: {sarcasm_precision:.4f}\")\n",
        "print(f\"  Recall:    {sarcasm_recall:.4f}\")\n",
        "print(f\"  F1-Score:  {sarcasm_f1:.4f}\")\n",
        "print(f\"  ROC-AUC:   {sarcasm_auc:.4f}\")\n",
        "\n",
        "print(f\"\\nConfusion Matrix:\")\n",
        "cm_sarcasm = confusion_matrix(sarcasm_labels, sarcasm_preds)\n",
        "print(f\"                Predicted\")\n",
        "print(f\"               Non-Sarc  Sarcastic\")\n",
        "print(f\"Actual Non-Sarc    {cm_sarcasm[0][0]:4d}      {cm_sarcasm[0][1]:4d}\")\n",
        "print(f\"Actual Sarcastic   {cm_sarcasm[1][0]:4d}      {cm_sarcasm[1][1]:4d}\")\n",
        "\n",
        "# ============================================================================\n",
        "# 9. EMOTION CLASSIFICATION METRICS\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"9. EMOTION CLASSIFICATION RESULTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "emotion_acc = accuracy_score(emotion_labels, emotion_preds)\n",
        "emotion_precision, emotion_recall, emotion_f1, _ = precision_recall_fscore_support(\n",
        "    emotion_labels, emotion_preds, average='macro'\n",
        ")\n",
        "emotion_precision_w, emotion_recall_w, emotion_f1_w, _ = precision_recall_fscore_support(\n",
        "    emotion_labels, emotion_preds, average='weighted'\n",
        ")\n",
        "\n",
        "print(f\"\\nEmotion Classification Metrics:\")\n",
        "print(f\"  Accuracy:           {emotion_acc:.4f}\")\n",
        "print(f\"  Macro Precision:    {emotion_precision:.4f}\")\n",
        "print(f\"  Macro Recall:       {emotion_recall:.4f}\")\n",
        "print(f\"  Macro F1-Score:     {emotion_f1:.4f}\")\n",
        "print(f\"  Weighted F1-Score:  {emotion_f1_w:.4f}\")\n",
        "\n",
        "print(f\"\\nPer-Emotion Performance:\")\n",
        "emotion_report = classification_report(\n",
        "    emotion_labels, emotion_preds,\n",
        "    target_names=emotion_classes,\n",
        "    digits=4\n",
        ")\n",
        "print(emotion_report)\n",
        "\n",
        "# ============================================================================\n",
        "# 10. CONFUSION ANALYSIS\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"10. CONFUSION ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "cm_emotion = confusion_matrix(emotion_labels, emotion_preds)\n",
        "\n",
        "# Find top confusions\n",
        "confusions = []\n",
        "for i in range(len(emotion_classes)):\n",
        "    for j in range(len(emotion_classes)):\n",
        "        if i != j and cm_emotion[i][j] > 0:\n",
        "            confusions.append((emotion_classes[i], emotion_classes[j], cm_emotion[i][j]))\n",
        "\n",
        "confusions.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "print(\"\\nMost Common Misclassifications:\")\n",
        "for idx, (true_label, pred_label, count) in enumerate(confusions[:10], 1):\n",
        "    print(f\"  {idx:2d}. {true_label:15s} → {pred_label:15s}: {count:3d} times\")\n",
        "\n",
        "# ============================================================================\n",
        "# 11. SAMPLE PREDICTIONS\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"11. SAMPLE PREDICTIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "sample_indices = np.random.choice(len(X_test), min(8, len(X_test)), replace=False)\n",
        "\n",
        "print(\"\\nRandom Test Samples:\")\n",
        "for idx in sample_indices:\n",
        "    true_sarcasm = sarcasm_labels[idx]\n",
        "    pred_sarcasm = sarcasm_preds[idx]\n",
        "    true_emotion = emotion_classes[emotion_labels[idx]]\n",
        "    pred_emotion = emotion_classes[emotion_preds[idx]]\n",
        "    headline = X_test[idx]\n",
        "\n",
        "    sarcasm_match = \"✓\" if true_sarcasm == pred_sarcasm else \"✗\"\n",
        "    emotion_match = \"✓\" if true_emotion == pred_emotion else \"✗\"\n",
        "\n",
        "    print(f\"\\n  Headline: {headline}\")\n",
        "    print(f\"  Sarcasm - True: {true_sarcasm}, Pred: {pred_sarcasm} {sarcasm_match}\")\n",
        "    print(f\"  Emotion - True: {true_emotion}, Pred: {pred_emotion} {emotion_match}\")\n",
        "\n",
        "# ============================================================================\n",
        "# 12. TRAINING SUMMARY\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"12. TRAINING SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nModel: Simplified Multi-Task RoBERTa\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"Training samples: {len(X_train)}\")\n",
        "print(f\"Validation samples: {len(X_val)}\")\n",
        "print(f\"Test samples: {len(X_test)}\")\n",
        "print(f\"Epochs completed: {len(training_history)}\")\n",
        "\n",
        "print(f\"\\nFinal Test Results:\")\n",
        "print(f\"  Sarcasm Detection:\")\n",
        "print(f\"    - Accuracy: {sarcasm_acc:.4f}\")\n",
        "print(f\"    - F1-Score: {sarcasm_f1:.4f}\")\n",
        "print(f\"    - ROC-AUC:  {sarcasm_auc:.4f}\")\n",
        "print(f\"  Emotion Classification:\")\n",
        "print(f\"    - Accuracy:      {emotion_acc:.4f}\")\n",
        "print(f\"    - Macro F1:      {emotion_f1:.4f}\")\n",
        "print(f\"    - Weighted F1:   {emotion_f1_w:.4f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# 13. SAVE MODEL FOR WEB APP\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"13. SAVING MODEL FOR DEPLOYMENT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "save_dir = \"fixed_multitask_roberta\"\n",
        "\n",
        "import os\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Save tokenizer\n",
        "print(\"Saving tokenizer...\")\n",
        "tokenizer.save_pretrained(save_dir)\n",
        "\n",
        "# Save model weights\n",
        "print(\"Saving model weights...\")\n",
        "torch.save(model.state_dict(), os.path.join(save_dir, \"pytorch_model.bin\"))\n",
        "\n",
        "# Save config\n",
        "print(\"Saving model config...\")\n",
        "config = {\n",
        "    \"model_type\": \"roberta\",\n",
        "    \"num_emotions\": num_emotions,\n",
        "    \"hidden_size\": 768,\n",
        "    \"emotion_classes\": emotion_classes.tolist(),\n",
        "    \"emotion_mapping\": emotion_mapping\n",
        "}\n",
        "import json\n",
        "with open(os.path.join(save_dir, \"config.json\"), \"w\") as f:\n",
        "    json.dump(config, f, indent=4)\n",
        "\n",
        "# Save label encoder\n",
        "import joblib\n",
        "joblib.dump(label_encoder, os.path.join(save_dir, \"label_encoder.pkl\"))\n",
        "\n",
        "print(f\"\\nModel successfully saved to: {save_dir}/\")\n",
        "print(\"Ready for web app deployment!\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TRAINING COMPLETED SUCCESSFULLY!\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lopel9skKRtz",
        "outputId": "19fc1b01-824c-4707-ec51-50be291281ed"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "FIXED MULTI-TASK ROBERTA: SARCASM + EMOTION CLASSIFICATION\n",
            "================================================================================\n",
            "\n",
            "Using device: cuda\n",
            "GPU: Tesla T4\n",
            "\n",
            "================================================================================\n",
            "1. LOADING AND PREPARING DATA\n",
            "================================================================================\n",
            "Loaded dataset: 13478 rows\n",
            "\n",
            "Preprocessing text...\n",
            "\n",
            "Target Variables:\n",
            "  - is_sarcastic: Binary (0/1)\n",
            "  - emotion: 8 classes (reduced from 27)\n",
            "  Emotion classes: ['complex', 'humor', 'irony', 'negative', 'neutral', 'nostalgia', 'positive', 'surprise']\n",
            "\n",
            "Emotion Distribution:\n",
            "  complex        : 3952 samples (29.3%)\n",
            "  negative       : 3549 samples (26.3%)\n",
            "  positive       : 3407 samples (25.3%)\n",
            "  nostalgia      :  543 samples (4.0%)\n",
            "  irony          :  520 samples (3.9%)\n",
            "  surprise       :  518 samples (3.8%)\n",
            "  neutral        :  502 samples (3.7%)\n",
            "  humor          :  487 samples (3.6%)\n",
            "\n",
            "================================================================================\n",
            "2. CREATING DATA SPLITS\n",
            "================================================================================\n",
            "Dataset Split:\n",
            "  Train set: 10782 samples (80.0%)\n",
            "  Val set:   1348 samples (10.0%)\n",
            "  Test set:  1348 samples (10.0%)\n",
            "\n",
            "================================================================================\n",
            "3. TOKENIZING WITH ROBERTA\n",
            "================================================================================\n",
            "Loading RoBERTa tokenizer...\n",
            "Max sequence length: 128\n",
            "Creating datasets...\n",
            "Batch size: 32\n",
            "Training batches: 337\n",
            "Validation batches: 43\n",
            "Test batches: 43\n",
            "\n",
            "================================================================================\n",
            "4. BUILDING SIMPLIFIED MULTI-TASK MODEL\n",
            "================================================================================\n",
            "Loading simplified RoBERTa model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total parameters: 124,653,322\n",
            "Trainable parameters: 124,653,322\n",
            "\n",
            "================================================================================\n",
            "5. TRAINING CONFIGURATION\n",
            "================================================================================\n",
            "Epochs: 6\n",
            "Initial task weights - Emotion: 0.8, Sarcasm: 0.2\n",
            "Learning rates - RoBERTa: 2e-5, Sarcasm: 1e-4, Emotion: 5e-4\n",
            "Total training steps: 2022\n",
            "\n",
            "================================================================================\n",
            "6. STARTING TRAINING\n",
            "================================================================================\n",
            "\n",
            "Starting training...\n",
            "\n",
            "================================================================================\n",
            "EPOCH 1/6\n",
            "================================================================================\n",
            "  Batch  50/337 | Loss: 1.4941 | Sarcasm: 0.4581 | Emotion: 0.2075 | Weights: S(0.2) E(0.8)\n",
            "  Batch 100/337 | Loss: 1.3757 | Sarcasm: 0.5141 | Emotion: 0.2791 | Weights: S(0.2) E(0.8)\n",
            "  Batch 150/337 | Loss: 1.3635 | Sarcasm: 0.5831 | Emotion: 0.3442 | Weights: S(0.2) E(0.8)\n",
            "  Batch 200/337 | Loss: 0.9668 | Sarcasm: 0.6345 | Emotion: 0.4003 | Weights: S(0.2) E(0.8)\n",
            "  Batch 250/337 | Loss: 0.8318 | Sarcasm: 0.6704 | Emotion: 0.4494 | Weights: S(0.2) E(0.8)\n",
            "  Batch 300/337 | Loss: 0.9055 | Sarcasm: 0.6999 | Emotion: 0.4867 | Weights: S(0.2) E(0.8)\n",
            "\n",
            "Epoch 1 Summary:\n",
            "  Training   -> Loss: 1.1673, Sarcasm: 0.7143, Emotion: 0.5091\n",
            "  Validation -> Loss: 0.5481, Sarcasm: 0.8620, Emotion: 0.7300\n",
            "  Emotion weight: 0.80\n",
            "  ✓ New best model! Emotion accuracy: 0.7300\n",
            "\n",
            "================================================================================\n",
            "EPOCH 2/6\n",
            "================================================================================\n",
            "  Batch  50/337 | Loss: 0.7008 | Sarcasm: 0.8806 | Emotion: 0.7675 | Weights: S(0.3) E(0.7)\n",
            "  Batch 100/337 | Loss: 0.5832 | Sarcasm: 0.8875 | Emotion: 0.7612 | Weights: S(0.3) E(0.7)\n",
            "  Batch 150/337 | Loss: 0.4340 | Sarcasm: 0.8838 | Emotion: 0.7698 | Weights: S(0.3) E(0.7)\n",
            "  Batch 200/337 | Loss: 0.2534 | Sarcasm: 0.8875 | Emotion: 0.7784 | Weights: S(0.3) E(0.7)\n",
            "  Batch 250/337 | Loss: 0.3086 | Sarcasm: 0.8891 | Emotion: 0.7837 | Weights: S(0.3) E(0.7)\n",
            "  Batch 300/337 | Loss: 0.4331 | Sarcasm: 0.8918 | Emotion: 0.7893 | Weights: S(0.3) E(0.7)\n",
            "\n",
            "Epoch 2 Summary:\n",
            "  Training   -> Loss: 0.5115, Sarcasm: 0.8933, Emotion: 0.7942\n",
            "  Validation -> Loss: 0.3942, Sarcasm: 0.8984, Emotion: 0.8131\n",
            "  Emotion weight: 0.70\n",
            "  ✓ New best model! Emotion accuracy: 0.8131\n",
            "\n",
            "================================================================================\n",
            "EPOCH 3/6\n",
            "================================================================================\n",
            "  Batch  50/337 | Loss: 0.2355 | Sarcasm: 0.9331 | Emotion: 0.8744 | Weights: S(0.4) E(0.6)\n",
            "  Batch 100/337 | Loss: 0.2624 | Sarcasm: 0.9313 | Emotion: 0.8747 | Weights: S(0.4) E(0.6)\n",
            "  Batch 150/337 | Loss: 0.2657 | Sarcasm: 0.9304 | Emotion: 0.8683 | Weights: S(0.4) E(0.6)\n",
            "  Batch 200/337 | Loss: 0.2323 | Sarcasm: 0.9319 | Emotion: 0.8691 | Weights: S(0.4) E(0.6)\n",
            "  Batch 250/337 | Loss: 0.2364 | Sarcasm: 0.9345 | Emotion: 0.8699 | Weights: S(0.4) E(0.6)\n",
            "  Batch 300/337 | Loss: 0.2498 | Sarcasm: 0.9356 | Emotion: 0.8727 | Weights: S(0.4) E(0.6)\n",
            "\n",
            "Epoch 3 Summary:\n",
            "  Training   -> Loss: 0.2936, Sarcasm: 0.9364, Emotion: 0.8732\n",
            "  Validation -> Loss: 0.3415, Sarcasm: 0.9184, Emotion: 0.8368\n",
            "  Emotion weight: 0.60\n",
            "  ✓ New best model! Emotion accuracy: 0.8368\n",
            "\n",
            "================================================================================\n",
            "EPOCH 4/6\n",
            "================================================================================\n",
            "  Batch  50/337 | Loss: 0.2036 | Sarcasm: 0.9556 | Emotion: 0.9200 | Weights: S(0.5) E(0.5)\n",
            "  Batch 100/337 | Loss: 0.1748 | Sarcasm: 0.9591 | Emotion: 0.9237 | Weights: S(0.5) E(0.5)\n",
            "  Batch 150/337 | Loss: 0.2677 | Sarcasm: 0.9535 | Emotion: 0.9196 | Weights: S(0.5) E(0.5)\n",
            "  Batch 200/337 | Loss: 0.0980 | Sarcasm: 0.9553 | Emotion: 0.9203 | Weights: S(0.5) E(0.5)\n",
            "  Batch 250/337 | Loss: 0.1955 | Sarcasm: 0.9569 | Emotion: 0.9197 | Weights: S(0.5) E(0.5)\n",
            "  Batch 300/337 | Loss: 0.1481 | Sarcasm: 0.9573 | Emotion: 0.9193 | Weights: S(0.5) E(0.5)\n",
            "\n",
            "Epoch 4 Summary:\n",
            "  Training   -> Loss: 0.1774, Sarcasm: 0.9570, Emotion: 0.9212\n",
            "  Validation -> Loss: 0.3239, Sarcasm: 0.9251, Emotion: 0.8531\n",
            "  Emotion weight: 0.50\n",
            "  ✓ New best model! Emotion accuracy: 0.8531\n",
            "\n",
            "================================================================================\n",
            "EPOCH 5/6\n",
            "================================================================================\n",
            "  Batch  50/337 | Loss: 0.0747 | Sarcasm: 0.9750 | Emotion: 0.9494 | Weights: S(0.6) E(0.4)\n",
            "  Batch 100/337 | Loss: 0.1126 | Sarcasm: 0.9706 | Emotion: 0.9481 | Weights: S(0.6) E(0.4)\n",
            "  Batch 150/337 | Loss: 0.0967 | Sarcasm: 0.9717 | Emotion: 0.9502 | Weights: S(0.6) E(0.4)\n",
            "  Batch 200/337 | Loss: 0.0882 | Sarcasm: 0.9698 | Emotion: 0.9497 | Weights: S(0.6) E(0.4)\n",
            "  Batch 250/337 | Loss: 0.0606 | Sarcasm: 0.9704 | Emotion: 0.9493 | Weights: S(0.6) E(0.4)\n",
            "  Batch 300/337 | Loss: 0.1381 | Sarcasm: 0.9707 | Emotion: 0.9491 | Weights: S(0.6) E(0.4)\n",
            "\n",
            "Epoch 5 Summary:\n",
            "  Training   -> Loss: 0.1095, Sarcasm: 0.9710, Emotion: 0.9487\n",
            "  Validation -> Loss: 0.3084, Sarcasm: 0.9362, Emotion: 0.8583\n",
            "  Emotion weight: 0.40\n",
            "  ✓ New best model! Emotion accuracy: 0.8583\n",
            "\n",
            "================================================================================\n",
            "EPOCH 6/6\n",
            "================================================================================\n",
            "  Batch  50/337 | Loss: 0.0550 | Sarcasm: 0.9856 | Emotion: 0.9587 | Weights: S(0.6) E(0.4)\n",
            "  Batch 100/337 | Loss: 0.1949 | Sarcasm: 0.9822 | Emotion: 0.9597 | Weights: S(0.6) E(0.4)\n",
            "  Batch 150/337 | Loss: 0.1061 | Sarcasm: 0.9817 | Emotion: 0.9587 | Weights: S(0.6) E(0.4)\n",
            "  Batch 200/337 | Loss: 0.0440 | Sarcasm: 0.9795 | Emotion: 0.9595 | Weights: S(0.6) E(0.4)\n",
            "  Batch 250/337 | Loss: 0.0580 | Sarcasm: 0.9810 | Emotion: 0.9577 | Weights: S(0.6) E(0.4)\n",
            "  Batch 300/337 | Loss: 0.1453 | Sarcasm: 0.9808 | Emotion: 0.9600 | Weights: S(0.6) E(0.4)\n",
            "\n",
            "Epoch 6 Summary:\n",
            "  Training   -> Loss: 0.0800, Sarcasm: 0.9809, Emotion: 0.9602\n",
            "  Validation -> Loss: 0.3218, Sarcasm: 0.9392, Emotion: 0.8642\n",
            "  Emotion weight: 0.40\n",
            "  ✓ New best model! Emotion accuracy: 0.8642\n",
            "Loaded best model for final evaluation.\n",
            "\n",
            "================================================================================\n",
            "7. FINAL TEST SET EVALUATION\n",
            "================================================================================\n",
            "\n",
            "Test Loss: 0.3431\n",
            "\n",
            "================================================================================\n",
            "8. SARCASM DETECTION RESULTS\n",
            "================================================================================\n",
            "\n",
            "Sarcasm Detection Metrics:\n",
            "  Accuracy:  0.9444\n",
            "  Precision: 0.9617\n",
            "  Recall:    0.9124\n",
            "  F1-Score:  0.9364\n",
            "  ROC-AUC:   0.9870\n",
            "\n",
            "Confusion Matrix:\n",
            "                Predicted\n",
            "               Non-Sarc  Sarcastic\n",
            "Actual Non-Sarc     721        22\n",
            "Actual Sarcastic     53       552\n",
            "\n",
            "================================================================================\n",
            "9. EMOTION CLASSIFICATION RESULTS\n",
            "================================================================================\n",
            "\n",
            "Emotion Classification Metrics:\n",
            "  Accuracy:           0.8680\n",
            "  Macro Precision:    0.8225\n",
            "  Macro Recall:       0.8320\n",
            "  Macro F1-Score:     0.8267\n",
            "  Weighted F1-Score:  0.8685\n",
            "\n",
            "Per-Emotion Performance:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     complex     0.8894    0.8939    0.8917       396\n",
            "       humor     0.6600    0.6735    0.6667        49\n",
            "       irony     0.9020    0.8846    0.8932        52\n",
            "    negative     0.8917    0.9042    0.8979       355\n",
            "     neutral     0.6545    0.7200    0.6857        50\n",
            "   nostalgia     0.9412    0.9057    0.9231        53\n",
            "    positive     0.8865    0.8475    0.8666       341\n",
            "    surprise     0.7544    0.8269    0.7890        52\n",
            "\n",
            "    accuracy                         0.8680      1348\n",
            "   macro avg     0.8225    0.8320    0.8267      1348\n",
            "weighted avg     0.8695    0.8680    0.8685      1348\n",
            "\n",
            "\n",
            "================================================================================\n",
            "10. CONFUSION ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "Most Common Misclassifications:\n",
            "   1. positive        → complex        :  25 times\n",
            "   2. complex         → positive       :  19 times\n",
            "   3. positive        → negative       :  12 times\n",
            "   4. negative        → complex        :  10 times\n",
            "   5. complex         → surprise       :   9 times\n",
            "   6. negative        → positive       :   9 times\n",
            "   7. complex         → negative       :   8 times\n",
            "   8. positive        → neutral        :   8 times\n",
            "   9. humor           → negative       :   7 times\n",
            "  10. neutral         → humor          :   6 times\n",
            "\n",
            "================================================================================\n",
            "11. SAMPLE PREDICTIONS\n",
            "================================================================================\n",
            "\n",
            "Random Test Samples:\n",
            "\n",
            "  Headline: New Diet: Eat Only Kale, Become Immortal, Right\n",
            "  Sarcasm - True: 1, Pred: 1 ✓\n",
            "  Emotion - True: humor, Pred: humor ✓\n",
            "\n",
            "  Headline: Leader Delivers Genuine Apology for Policy Misstep\n",
            "  Sarcasm - True: 0, Pred: 0 ✓\n",
            "  Emotion - True: complex, Pred: complex ✓\n",
            "\n",
            "  Headline: Oh, learned wisdom by repeating the same mistake thrice\n",
            "  Sarcasm - True: 1, Pred: 1 ✓\n",
            "  Emotion - True: complex, Pred: complex ✓\n",
            "\n",
            "  Headline: Another \"Clean\" Coal Plant Approved\n",
            "  Sarcasm - True: 1, Pred: 1 ✓\n",
            "  Emotion - True: negative, Pred: negative ✓\n",
            "\n",
            "  Headline: School Replaces Teachers with Screens\n",
            "  Sarcasm - True: 0, Pred: 0 ✓\n",
            "  Emotion - True: negative, Pred: negative ✓\n",
            "\n",
            "  Headline: what your movements may reveal about how you'll get along with another person\n",
            "  Sarcasm - True: 0, Pred: 0 ✓\n",
            "  Emotion - True: neutral, Pred: complex ✗\n",
            "\n",
            "  Headline: Museum guide mixes up historical dates mid-tour\n",
            "  Sarcasm - True: 0, Pred: 0 ✓\n",
            "  Emotion - True: negative, Pred: negative ✓\n",
            "\n",
            "  Headline: Unemployment rises sharply during recession\n",
            "  Sarcasm - True: 0, Pred: 0 ✓\n",
            "  Emotion - True: irony, Pred: irony ✓\n",
            "\n",
            "================================================================================\n",
            "12. TRAINING SUMMARY\n",
            "================================================================================\n",
            "\n",
            "Model: Simplified Multi-Task RoBERTa\n",
            "Trainable parameters: 124,653,322\n",
            "Training samples: 10782\n",
            "Validation samples: 1348\n",
            "Test samples: 1348\n",
            "Epochs completed: 6\n",
            "\n",
            "Final Test Results:\n",
            "  Sarcasm Detection:\n",
            "    - Accuracy: 0.9444\n",
            "    - F1-Score: 0.9364\n",
            "    - ROC-AUC:  0.9870\n",
            "  Emotion Classification:\n",
            "    - Accuracy:      0.8680\n",
            "    - Macro F1:      0.8267\n",
            "    - Weighted F1:   0.8685\n",
            "\n",
            "================================================================================\n",
            "13. SAVING MODEL FOR DEPLOYMENT\n",
            "================================================================================\n",
            "Saving tokenizer...\n",
            "Saving model weights...\n",
            "Saving model config...\n",
            "\n",
            "Model successfully saved to: fixed_multitask_roberta/\n",
            "Ready for web app deployment!\n",
            "\n",
            "================================================================================\n",
            "TRAINING COMPLETED SUCCESSFULLY!\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# DOWNLOAD MODEL FILES FOR STREAMLIT\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PREPARING FILES FOR STREAMLIT DEPLOYMENT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "import shutil\n",
        "import joblib\n",
        "\n",
        "# Create a zip file with all model files\n",
        "streamlit_files_dir = \"streamlit_model_files\"\n",
        "os.makedirs(streamlit_files_dir, exist_ok=True)\n",
        "\n",
        "# Copy all necessary files\n",
        "print(\"Copying model files...\")\n",
        "shutil.copytree(\"fixed_multitask_roberta\", os.path.join(streamlit_files_dir, \"fixed_multitask_roberta\"), dirs_exist_ok=True)\n",
        "\n",
        "# Save the label encoder separately for easy loading\n",
        "joblib.dump(label_encoder, os.path.join(streamlit_files_dir, \"label_encoder.pkl\"))\n",
        "\n",
        "# Create a requirements.txt for Streamlit\n",
        "requirements = \"\"\"\n",
        "torch>=1.9.0\n",
        "transformers>=4.20.0\n",
        "streamlit>=1.22.0\n",
        "pandas>=1.3.0\n",
        "numpy>=1.21.0\n",
        "scikit-learn>=1.0.0\n",
        "joblib>=1.1.0\n",
        "\"\"\".strip()\n",
        "\n",
        "with open(os.path.join(streamlit_files_dir, \"requirements.txt\"), \"w\") as f:\n",
        "    f.write(requirements)\n",
        "\n",
        "# Create a quick test script\n",
        "test_script = \"\"\"\n",
        "# Quick test script for your model\n",
        "import torch\n",
        "import joblib\n",
        "from transformers import RobertaTokenizer\n",
        "from your_model_definition import SimpleMultiTaskRoBERTa\n",
        "\n",
        "def test_model():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Load tokenizer and model\n",
        "    tokenizer = RobertaTokenizer.from_pretrained('fixed_multitask_roberta')\n",
        "    label_encoder = joblib.load('label_encoder.pkl')\n",
        "\n",
        "    # Initialize model architecture\n",
        "    model = SimpleMultiTaskRoBERTa(num_emotions=len(label_encoder.classes_))\n",
        "    model.load_state_dict(torch.load('fixed_multitask_roberta/pytorch_model.bin', map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Test prediction\n",
        "    test_text = \"This is absolutely fantastic news!\"\n",
        "    inputs = tokenizer(test_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        sarcasm_logits, emotion_logits = model(inputs['input_ids'], inputs['attention_mask'])\n",
        "\n",
        "    sarcasm_pred = torch.argmax(sarcasm_logits, dim=1).item()\n",
        "    emotion_pred = label_encoder.inverse_transform([torch.argmax(emotion_logits, dim=1).item()])[0]\n",
        "\n",
        "    print(f\"Text: {test_text}\")\n",
        "    print(f\"Sarcastic: {bool(sarcasm_pred)}\")\n",
        "    print(f\"Emotion: {emotion_pred}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_model()\n",
        "\"\"\"\n",
        "\n",
        "with open(os.path.join(streamlit_files_dir, \"test_model.py\"), \"w\") as f:\n",
        "    f.write(test_script)\n",
        "\n",
        "# Create zip file\n",
        "print(\"Creating zip file...\")\n",
        "shutil.make_archive(\"streamlit_model_files\", 'zip', streamlit_files_dir)\n",
        "\n",
        "print(f\"\\n✅ All files ready! Download 'streamlit_model_files.zip' from the Colab file browser.\")\n",
        "print(\"📁 Files included:\")\n",
        "print(\"   - fixed_multitask_roberta/ (model weights, tokenizer, config)\")\n",
        "print(\"   - label_encoder.pkl\")\n",
        "print(\"   - requirements.txt\")\n",
        "print(\"   - test_model.py\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wt1L39kcZNhR",
        "outputId": "856c9a36-5110-4167-e892-68aec857da2a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "PREPARING FILES FOR STREAMLIT DEPLOYMENT\n",
            "================================================================================\n",
            "Copying model files...\n",
            "Creating zip file...\n",
            "\n",
            "✅ All files ready! Download 'streamlit_model_files.zip' from the Colab file browser.\n",
            "📁 Files included:\n",
            "   - fixed_multitask_roberta/ (model weights, tokenizer, config)\n",
            "   - label_encoder.pkl\n",
            "   - requirements.txt\n",
            "   - test_model.py\n"
          ]
        }
      ]
    }
  ]
}